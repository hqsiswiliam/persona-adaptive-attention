experiment:
  name: transformer_bert2gpt

transformer:
  encoder:
    using_pretrained: null
    hidden_size: 768
    num_attention_heads: 4
    num_hidden_layers: 4
    max_position_embeddings: 512
    model_type: bert
  decoder:
    using_pretrained: gpt2
#    hidden_size: 768
#    attention_probs_dropout_prob: 0.0
#    hidden_dropout_prob: 0.0
#    num_attention_heads: 12
#    num_hidden_layers: 12
#    max_position_embeddings: 512
    model_type: gpt2

tokenizer:
  vocab: gpt2

training:
  batch_size: 4
  lr: !!float 1e-6
  epoch: 30
  seed: 36
  optimizer: adam
  gradient_clip: 0.1