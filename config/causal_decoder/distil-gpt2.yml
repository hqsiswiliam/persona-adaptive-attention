experiment:
  name: distil-gpt2
causal_decoder:
  decoder_type: distilgpt2
  preserve_weight: Yes

tokenizer:
  vocab: distilgpt2

training:
  batch_size: 4
  lr: !!float 1e-5
  epoch: 30
  seed: 36
  optimizer: adam
  gradient_clip: 0.1